{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Credit Risk Modeling**\n",
    "\n",
    "#### **Name: Bernardo Raimundo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "* [1. Introduction](#1)\n",
    "* [2. Importing Libraries](#2)\n",
    "* [3. Data Preparation](#3)\n",
    "* [4. Dealing with potential Outliers](#4)\n",
    "* [5. Encoding Variables](#5)\n",
    "* [6. Machine Learning Pipeline](#6)\n",
    "* [7. Performance Metrics](#7)\n",
    "* [8. Undersample Approach](#8)\n",
    "* [9. Oversample Approach](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Introduction**\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "\n",
    "Credit risk scoring methods are widely used for evaluating potential loan applications in both financial and banking institutions. These procedures allow banks to describe the likelihood of a default over a particular time horizon. \n",
    "\n",
    "Financial institutions deploy models that consider the credit history of the borrowers and the inputs from their own economic stress scenarios to measure credit risk. Keen awareness is needed to identify, measure, monitor and control credit risk, guarantee sufficient capital against these risks and adequate compensation for the risks incurred.\n",
    "\n",
    "When an institution receives a loan application, a decision regarding the approval of the loan has to be made considering the applicant’s profile. There are two types of risks assoiciated with this decision:\n",
    "\n",
    "- If the applicant is likely to repay the loan, then not approving the loan results in a loss of business to an instituiton;\n",
    "\n",
    "- If the applicant is not likely to repay the loan, i.e. he/she is likely to default, then approving the loan may lead to a financial.\n",
    "\n",
    "The data for this specific scenario is provided by LendingClub, a P2P lending company that provides loans without the need of any financial intermdediation and/or collateral. Investors are responsible for their own decision on lending. Even though the company has fitter borrowers who meet the minimum requirements, there is still a huge risk of non-repayment. This is the problem that this project works on by making the prediction of loan status for these applicants.\n",
    "\n",
    "The data contains information about past loan applicants and whether they ‘defaulted’ or not. \n",
    "\n",
    "The aim is to identify patterns, which may serve as key information to either deny a loan to an applicant, reduce the amount of loan or even lending at higher interest rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main tasks of this research:**\n",
    "\n",
    "- Make a preliminary statistical analysis of the credit dataset\n",
    "\n",
    "- Develop a several models to predict the probability of default evaluating them using different performance metrics\n",
    "\n",
    "- Utilize model combination (Stacking Generalization Approach) and compare it to the more traditional credit scoring methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook covers feature engeneering and modeling a logistic regression using stochastic gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Importing Libraries**\n",
    "\n",
    "<a id=\"2\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10580\\4166711175.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, learning_curve, cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.pipeline import  Pipeline\n",
    "from imblearn.pipeline import Pipeline as imb_pipeline\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional configuration features for easier vizualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.options.mode.chained_assignment = None \n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking Working Directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd() ### Check Working Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('1. dataset_for_feature_engeneering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['Unnamed: 0'],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Data Preparation**\n",
    "\n",
    "<a id=\"3\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we chose the potential variables for the modeling phase some more data preparation is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check which variables have missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(df_train):\n",
    "    total = df_train.isnull().sum()\n",
    "    percent = (df_train.isnull().sum()/df_train.isnull().count()*100)\n",
    "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in df_train.columns:\n",
    "        dtype = str(df_train[col].dtype)\n",
    "        types.append(dtype)\n",
    "    tt['Types'] = types\n",
    "    return(np.transpose(tt))\n",
    "\n",
    "missing_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We still have some missing values but these represent less than 10% of the total data. These values will be median imputed.** \n",
    "\n",
    "**This step will be executed when we build the machine learning pipeline later on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Dealing with potential Outliers**\n",
    "\n",
    "<a id=\"4\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are values that fall of outside of the overall patern of a data set.\n",
    "\n",
    "To deal with these outliers we decided to use the interquantile rule by:\n",
    "\n",
    "- Calculate the Interquantile range (IQR) for the data\n",
    "\n",
    "- Multiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers).\n",
    "\n",
    "- Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.\n",
    " \n",
    "- Subtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier.\n",
    "\n",
    "Outlier Removal Tradeoff:\n",
    "\n",
    "We have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.\n",
    "\n",
    "The Tradeoff: The lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_variables = list(df_train.select_dtypes('number').columns)\n",
    "\n",
    "print(f\"Numerical columns are: {numerical_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().apply(lambda s: s.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(40, 30))\n",
    "\n",
    "for var, subplot in zip(df_train[numerical], ax.flatten()):\n",
    "    \n",
    "    sns.boxplot(df_train[var], ax=subplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are unusual values in a datset, and they can distort statistical analyses and violate their assumptions. \n",
    "\n",
    "Analysts / Data Scientits need to confront outliers and be forced to make decisions about what to do with them. Given the problems they can cause, you might think that it’s best to remove them from your data. But, that’s not always the case. We should only remove an outlier if we are able to justify its exclusion. \n",
    "\n",
    "As mentioned in the first notebook both annual income and dti are variables that present rather extreme outliers. The removal of these outliers goes as follows:\n",
    "\n",
    "- Annual income: Values above 1000000 were dropped\n",
    "- Dti: Values above 60 were dropped\n",
    "- Revol_Bal: Values above 1000000 were dropped\n",
    "- Fico_score: Values bellow 249.5 were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = df_train[df_train['annual_inc'] > 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = df_train1[df_train1['dti'] > 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train4 = df_train[df_train['revol_bal'] > 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train5 = df_train1[df_train1['fico_score'] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = df_train[df_train['annual_inc'] <= 1000000]\n",
    "\n",
    "df_train2 = df_train1[df_train1['dti'] <= 60]\n",
    "\n",
    "df_train3 = df_train2[df_train2['revol_bal'] <= 1000000]\n",
    "\n",
    "df_train4 = df_train3[df_train3['fico_score'] > 0]\n",
    "\n",
    "df_train5 = df_train4[df_train4['dti'] > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train5 = df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Encoding Variables**\n",
    "\n",
    "<a id=\"5\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the target variable**\n",
    "\n",
    "Many of our model variables are categorical and these need to be properly encoded for modeling. Bellow we convert our target variable and present the correlations with the remaining features. \n",
    "\n",
    "We can see a high correlation with fico score, last payment amount, term and interest rate which makes sense as these features are essential when granitng a loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['loans'] = df_train['loan_status'].map({'Charged Off': 1, 'Fully Paid': 0})\n",
    "\n",
    "df_train.drop(['loan_status'],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = df_train.corrwith(df_train['loans'])\n",
    "\n",
    "df_train1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know create the target variable and copy of the dataset for modeling purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modeling dataset from the original dataset\n",
    "\n",
    "X = df_train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Target Variable\n",
    "\n",
    "y = X['loans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into a train and test set**\n",
    "\n",
    "For this research, the data was splitted in 80% for training and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Target from the training\n",
    "\n",
    "X_train = X_train.drop(['loans'],1)\n",
    "\n",
    "X_test = X_test.drop(['loans'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Building a Machine Learning Pipeline**\n",
    "\n",
    "<a id=\"6\"></a>\n",
    "\n",
    "**Modeling Logistic Regression using Stochastic Gradient Descent**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning pipeline is a way to automate the workflow enabling data to be transformed and correlated into a model that can then be analyzed to achieve outputs. This type of ML pipeline makes the process of inputting data into the ML model fully automated.\n",
    "\n",
    "For this particular notebook we are going to focus on modeling **Logistic Regression using Stochastic Gradient Descent**\n",
    "\n",
    "Logistic Regression model is a statistical method utilized in machine learning to assess the relationship between a dependent categorical variable (output) and one or more independent variables (predictors) by employing a logistic function to evaluate the probabilities.\n",
    "\n",
    "The Logistic Regression is trained using Stochastic Gradient Descent (SGD) which provides several advantages:\n",
    "\n",
    "- It is computationally faster and it works well on larger datasets\n",
    "- It is easier to fit into memory due to a single training sample being processed by the network\n",
    "\n",
    "The Pipeline that we are going to build is going to do the following steps:\n",
    "\n",
    "1) Deal with missing values: All missing values were median imputed;\n",
    "\n",
    "2) Encode categorical varaibles using Ordinal Encoding and One hot Encoding. Bellow we present our reasoning:\n",
    "\n",
    "There are two types of categorical varables in our dataset:\n",
    "\n",
    "- **Ordinal variables:** The levels of the variable follow a specific order - Grade variable we know that: **A>B>C>D>E>F>G**. To deal with this we map each grade feature with an interger respecting said ordering;\n",
    "\n",
    "- **Categorical variables**: The levels of the variables do not follow a specific order. Mapping these levels will introduce an order which in this case it is not desirable. We stored these variables in a list and then we use the OneHotEncoder library\n",
    "\n",
    "\n",
    "3) Standardize features by removing the mean and scaling to unit variance using StandardScaler\n",
    "\n",
    "4) PCA: Due to the large size of the dataset we performed Principal Componenet Analysis as it helps to deal with multicolienarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = list(X_train.select_dtypes('object').columns)\n",
    "\n",
    "print(f\"Categorical columns are: {categorical_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_variables = list(X_train.select_dtypes('number').columns)\n",
    "\n",
    "print(f\"Numerical columns are: {numerical_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make two list for categorical varaibles making implementation easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding_variables = ['home_ownership', 'purpose', 'addr_state', \"verification_status\", \"application_type\"]\n",
    "\n",
    "ordinal_encoding_variables = [\"grade\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numerical Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "numeric_col_transformer = Pipeline(steps=[  \n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    (\"scaler\",scaler),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ordinal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_col_transformer = Pipeline(steps=[\n",
    "    ('ordinalencoder', OrdinalEncoder()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One Hot Encoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_col_transformer = Pipeline(steps=[\n",
    "   ('onehotencoder', OneHotEncoder(sparse=False, handle_unknown = \"ignore\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = ColumnTransformer(transformers=[\n",
    "    (\"numerical\",numeric_col_transformer, numerical_variables),\n",
    "    (\"categorical_ordinal\", ordinal_col_transformer, ordinal_encoding_variables),\n",
    "    (\"categorical_onehot\", one_hot_col_transformer, one_hot_encoding_variables),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  SGDClassifier(loss='log',random_state=42,n_jobs=-1,warm_start=True, shuffle = True, class_weight = \"balanced\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(n_components = 5)\n",
    "\n",
    "pipeline_sgdlogreg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessing_pipeline),\n",
    "    (\"pca\", pca),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "pipeline_sgdlogreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present bellow a simple flowchart of our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_config(display=\"diagram\")\n",
    "\n",
    "pipeline_sgdlogreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision', \"recall\", \"f1\"]\n",
    "\n",
    "scores = cross_validate(pipeline_sgdlogreg, X_train, y_train, scoring=scoring, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"train_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"test_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"train_precision\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"test_precision\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"train_recall\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"test_recall\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"train_f1\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"test_f1\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The AUC Score is:\", roc_auc_score(y_train, pipeline_sgdlogreg.predict_proba(X_train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The AUC Score is:\", roc_auc_score(y_test, pipeline_sgdlogreg.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Stratified K Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. When a specific value for k is chosen, it may be used in place of k in the reference to the model.\n",
    "\n",
    "The general procedure of k fold cross validation goes as follows:\n",
    "\n",
    "- Shuffle the dataset randomly.\n",
    "- Pick a number of folds. For this research we considered K = 5 folds where 1 fold will be the test set and the remaining k-1 the training set\n",
    "- Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration\n",
    "- Validate the results on the test set by taking the average score\n",
    "\n",
    "There are many types of cross-validation procedures but for this scenatio we are going to use Stratified K Fold Cross Validation  which is an extension of the cross-validation technique used for classification problems. \n",
    "\n",
    "It maintains the same class ratio throughout the K folds as the ratio in the original dataset. Ideal for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KF = StratifiedKFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Hyperparameter Tunning using GridSearchCV**\n",
    "\n",
    "When designing a machine learning model, we will be presented with different choices as to how to define your model architecture. \n",
    "\n",
    "Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. \n",
    "\n",
    "Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n",
    "\n",
    "This process usually goes as follows:\n",
    "- Define a model\n",
    "- Define the range of possible values for all hyperparameters (parameter grid)\n",
    "- Define a method for sampling hyperparameter values\n",
    "- Define an evaluative criteria to evaluate the model\n",
    "- Define a cross-validation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_sgdlogreg = {}\n",
    "    \n",
    "param_grid_sgdlogreg['model__alpha'] = [10**-3, 10**-2, 10**-1]\n",
    "param_grid_sgdlogreg['model__penalty'] = ['l1', 'l2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sgdlogreg = GridSearchCV(pipeline_sgdlogreg, param_grid_sgdlogreg,\n",
    "                              scoring = 'f1', pre_dispatch = 3,\n",
    "                              n_jobs = -1, cv = KF, verbose = 10, return_train_score = True, error_score = \"raise\")\n",
    "\n",
    "search = grid_sgdlogreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(search.cv_results_[\"params\"]),pd.DataFrame(search.cv_results_[\"mean_test_score\"], columns=[\"f1\"])],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Estimator and Fitting the model considering the results from GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = search.best_estimator_\n",
    "\n",
    "# fitting the model for grid search \n",
    "\n",
    "clf.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions on both the training and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions_train = clf.predict(X_train)\n",
    "\n",
    "grid_predictions_test =  clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Performance Metrics**\n",
    "\n",
    "<a id=\"7\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "Performance metrics are a part of every machine learning problem. Wether we are dealing with a classification or regression problem performance metrics are used to monitor and measure the performance of a model during training and testing. Since we are dealing with a classification problem the following performance metrics will be used:\n",
    "\n",
    "- **Confusion Matrix:** Table layout which allows the visualization of the performance of an algorithm. A confusion matrix gives us four important measures:\n",
    "\n",
    "    - True Positives (TP) – Default situations that are correctly classified as default.\n",
    "    - True Negatives (TN) – Non-default situations that are correctly classified as non-default.\n",
    "    - False Positives (FP) - Non-default applicants, but the model classified / predicted them as default. In statistics this is known as a Type I error. (a))\n",
    "    - False Negatives (FN) – Default applicants, but the model classified / prediced them as non-default. In statistics this is known as a Type II error. (b))\n",
    "    \n",
    "    \n",
    "a)  Type I Error (False Positive, FP): When the model predicts as defaulter but the applicant does not default (Affects profitability)\n",
    "\n",
    "b)  Type II Error (False Negative, FN): Model predicts applicant as a non defaulter but he actually defaulted (affects losses and provisions)\n",
    "\n",
    "False Positive errors affects profitability of the lender (the model predicts default but it was a good applicant) whereas the False Negative cases affects the risk side of the business (affecting losses and provisions) as the model predicits non default but the applicant defaulted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Accuracy:** Accuracy simply refers to the number of correct predicitions divided by the total number of predictions.It is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Accuracy =  \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision Score**: Precision measures how accurately the model can capture default i.e out of the total predicted default cases, how many turned out to be default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Precision =  \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Recall/Sensitivity Score:** Recall or Sensitivity measures out of all the actual default cases; how many the model could predict correctly as default. \n",
    "\n",
    "     - It is known as the **True Positive Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Recall =  \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Specificity:** It measures the proportion of correctly identified Non Defaults over the total of Non Default prediction made by the model.\n",
    "\n",
    "    -  Also known as **True Negative Rate**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Specificity =  \\frac{TN}{TN+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **F1 Score:** : F1 Score represents a balance between precision and recall. Mathematically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F1  Score = 2 * \\frac {(precision * recall)}{(precision + recall)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ROC Curve and AUC Score:** ROC is a probability curve and AUC represent the degree or measure of separability. It tells how much the model is capable of distinguishing between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical and somewhat overlooked part of classification is deciding whether to prioritize precision or recall. This is more of a business question than a data science one and requires that we have a clear idea of our objective as well as how the costs of false positives compare to those of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a funtion for the performance metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performanceMetricsDF(metricsObj, yTrain, yPredTrain, yTest, yPredTest):\n",
    "    \n",
    "  measures_list = ['ACCURACY','PRECISION', 'RECALL', \"F1 SCORE\"]\n",
    "\n",
    "  train_results = [metricsObj.accuracy_score(yTrain, yPredTrain),\n",
    "                metricsObj.precision_score(yTrain, yPredTrain),\n",
    "                metricsObj.recall_score(yTrain, yPredTrain),\n",
    "                metricsObj.f1_score(yTrain, yPredTrain)]\n",
    "    \n",
    "  test_results = [metricsObj.accuracy_score(yTest, yPredTest),\n",
    "                metricsObj.precision_score(yTest, yPredTest),\n",
    "                metricsObj.recall_score(yTest, yPredTest),\n",
    "                metricsObj.f1_score(yTest, yPredTest)]\n",
    "\n",
    "  resultsDF = pd.DataFrame({'Measure': measures_list, 'Train': train_results, 'Test':test_results})\n",
    "    \n",
    "  return(resultsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a funtion for the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    " \n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(cf,annot=box_labels, fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "    bottom, top = ax.get_ylim() # These two lines were added due to bug on current Seaborn version\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5) #\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = metrics.confusion_matrix(y_test, grid_predictions_test)\n",
    "\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "categories = ['Non Default', 'Default']\n",
    "\n",
    "make_confusion_matrix(cf, group_names=labels,categories=categories, cmap='Blues', figsize = (15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframe with results for the Positive Class (Predict Default)**\n",
    "\n",
    "This Dataframe compiles the information of the positive class for the train and test set using the results of both classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = performanceMetricsDF(metrics, y_train, grid_predictions_train, y_test, grid_predictions_test)\n",
    "\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the confusion matrix of the SGD Logistic Regression, the model performed rather well in the accuracy metric achieving a score of 80%. \n",
    "\n",
    "**Why did this happen?**\n",
    "\n",
    "We need to take into consideration that the dataset being used is severly imbalanced and thus accuracy should serve as a complement of the performance metrics and not used alone. Accuracy simply refers to the proportion of correctly classified instances. It is usually the first metric you look at when evaluating a model. Since we are dealing with imbalanced data accuracy can be misleading, a model can predict the value of the dominant class for all predictions and achieve a high accuracy score. This is known as the accuracy paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function that computes the ROC curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "FP,TP,thresholds = metrics.roc_curve(y_test,clf.decision_function(X_test))\n",
    "\n",
    "plt.plot(FP,TP,label=\"ROC\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--k', lw=1)\n",
    "\n",
    "plt.title(\"ROC Curve - SGD Logistic Regression\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate / 1-Specificity\")\n",
    "\n",
    "plt.ylabel(\"True Positive Rate / Recall\")\n",
    "\n",
    "plt.legend(loc = \"lower right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The AUC Score for the train is:\", roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "\n",
    "print(\"The AUC Score for the test is:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally. The model can be evaluated on the training dataset and on a validation dataset after each update during training and plots of the measured performance can created to show learning curves.\n",
    "\n",
    "It is an additional metric that allows to see if our algorithm is underfitting or overfitting\n",
    "\n",
    "Code presented bellow is based on: https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    array = np.linspace(1, int(X.shape[0]*0.8))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = \"roc_auc\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color='red',\n",
    "             label='Training score')\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color='green',\n",
    "             label='Validation score')\n",
    "\n",
    "    plt.legend(loc = 'best')\n",
    "    return plt\n",
    "\n",
    "\n",
    "g = plot_learning_curve(clf, \"Learning curve\", X_train, y_train, cv=KF,ylim=(0, 1), n_jobs = -1,\n",
    "                  train_sizes=np.linspace(0.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Balancing the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our dataset and find the amount of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cases = np.array(df_train[df_train.loans == 1].index)\n",
    "\n",
    "len(default_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_default_cases = np.array(df_train[df_train.loans == 0].index)\n",
    "\n",
    "len(non_default_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced datasets are common problems in most classsification problems. Bellow we present some techniques used to deal with this issue:\n",
    "\n",
    "**1) Collect more data:**\n",
    "\n",
    "More information allow us to have a more complete and balanced perspective of our dataset\n",
    "\n",
    "**2) Changing the performance metrics**\n",
    "\n",
    "Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.\n",
    "\n",
    "There are metrics that have been designed to tell you a more compelte story when working with imbalanced classes. These metrics, that were presented earlier, include: \n",
    "\n",
    " - Precision: A measure of a classifiers exactness\n",
    "\n",
    " - Recall: A measure of a classifiers completeness \n",
    "\n",
    " - F1 Score: A weighted average of precision and recall\n",
    "\n",
    "\n",
    "**3) Using resampling techniques**\n",
    "\n",
    "\n",
    "The dataset can be changed to have more balanced data. This is known as sampling your dataset and there are two main methods that you can use to even-up the classes:\n",
    "\n",
    "   - Add copies of instances from the under-represented class called over-sampling\n",
    "\n",
    "   - Delete instances from the over-represented class, this is known as under-sampling.\n",
    "   \n",
    "   \n",
    "Other techniques include;\n",
    "\n",
    "**4) Generate Synthetic Samples**\n",
    "\n",
    "A technique similar to sampling is to create synthetic samples. Here we will use imblearn’s SMOTE or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n",
    "\n",
    "\n",
    "**5) Cost Sensitive Learning**\n",
    "\n",
    "A subfield of machine learning that takes the costs of prediction errors (and potentially other costs) into account when training a machine learning model. It is a field of study that is closely related to the field of imbalanced learning that is concerned with classification on datasets with a skewed class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Undersample and Oversample Approach**\n",
    "\n",
    "<a id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using undersampling we randomly selected the same amount of non-default cases as the default ones and created a new dataset. With the down-sized data, we then proceed to re-train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using oversample we randomly selected the same amount of default cases as the non-default ones and created a new dataset. With the down-sized data, we then proceed to re-train the model.\n",
    "\n",
    "When using the k-fold cross-validation is to directly split the data into 5 folds. \n",
    "\n",
    "In general,  k-fold cross-validation performance evaluation method relies on the assumption that - each fold data is a representative sample of the main data and reflects the class distribution of the target feature in the main data.\n",
    "\n",
    "However, applying this rule to an imbalanced classification problem poses a distribution problem that might result in a biased estimate or overfitting in favor of the majority class. The correct use of the k-fold cross-validation in an imbalanced class distribution problem, requires:\n",
    "\n",
    "   - That each k-fold data is stratified to capture the imbalanced class distribution of the target feature in the main data. This can be achieved using the stratified k-fold cross-validation;\n",
    "   \n",
    "   - That, at each cross-validation evaluation, only the training set is oversampled (using synthetic minority oversampling technique or other class balancing techniques). This can be achieved using a machine learning pipeline. Setting a pipeline helps prevents data leakage;\n",
    "   \n",
    "   - That, at each cross-validation evaluation, the test data is not oversampled i.e it is unaffected by the oversampling, though it maintains the imbalanced class distribution of the target feature as in the main data;\n",
    "   \n",
    "   - That the oversampling is never done on the main data but the training data set, during each k-fold cross-validation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collections.Counter(y_train))\n",
    "\n",
    "sns.countplot(x = \"loans\", data=pd.DataFrame(data={'loans':y_train}))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under = RandomUnderSampler(sampling_strategy=0.5, random_state = 42)\n",
    "\n",
    "over = RandomOverSampler(sampling_strategy=\"minority\", random_state = 42)\n",
    "\n",
    "X_train_rus, y_train_rus = under.fit_resample(X_train, y_train)\n",
    "\n",
    "X_train_final, y_train_final = over.fit_resample(X_train_rus, y_train_rus)\n",
    "\n",
    "print(collections.Counter(y_train_final))\n",
    "\n",
    "sns.countplot(x = \"loans\", data=pd.DataFrame(data={'loans':y_train_final}))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a pipeline**\n",
    "\n",
    "Note: The pipeline used for sampling is from the imb library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rus = SGDClassifier(loss='log',random_state=42,n_jobs=-1,warm_start=True, shuffle = True, class_weight = \"balanced\", alpha = 0.01)\n",
    "\n",
    "pipeline_sgdlogreg_rus = imb_pipeline(steps=[\n",
    "    ('preprocessor', preprocessing_pipeline),\n",
    "    (\"under\", under),\n",
    "    (\"over\", over),\n",
    "    (\"pca\", pca),\n",
    "    (\"model\", model_rus)\n",
    "])\n",
    "\n",
    "pipeline_sgdlogreg_rus.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions_train_rus = pipeline_sgdlogreg_rus.predict(X_train)\n",
    "\n",
    "grid_predictions_test_rus =  pipeline_sgdlogreg_rus.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = metrics.confusion_matrix(y_test, grid_predictions_test_rus)\n",
    "\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "categories = ['Non Default', 'Default']\n",
    "\n",
    "make_confusion_matrix(cf, group_names=labels,categories=categories, cmap='Blues', figsize = (15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframe with results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = performanceMetricsDF(metrics, y_train, grid_predictions_train_rus, y_test, grid_predictions_test_rus)\n",
    "\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "probs = pipeline_sgdlogreg_rus.predict_proba(X_test)\n",
    "\n",
    "FP,TP,thresholds = metrics.roc_curve(y_test,probs[:,1])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--k', lw=1)\n",
    "\n",
    "plt.plot(FP,TP,label=\"ROC\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "cutoff=np.argmax(np.abs(TP-FP))\n",
    "\n",
    "plt.title(\"ROC Curve of the SGD Logistic Regression\")\n",
    "\n",
    "plt.legend(loc = \"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The AUC Score is:\", roc_auc_score(y_test, pipeline_sgdlogreg_rus.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
